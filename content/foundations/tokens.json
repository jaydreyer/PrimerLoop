{
  "subject_slug": "foundations",
  "concept_slug": "tokens",
  "track": "FOUNDATIONS",
  "difficulty": "beginner",
  "version": 1,
  "title": "Tokens: How Text Becomes Numbers",
  "prerequisites": ["what-is-an-llm"],
  "lesson": {
    "why_it_matters": "LLMs do not read words. They operate on tokens. Tokens determine cost, context limits, and how meaning is represented internally. If you understand tokens, you understand how language enters the system.",
    "core_idea": "A token is a chunk of text that a model processes as a unit. Before text is fed into an LLM, it is broken into tokens and converted into numbers. The model predicts the next token, not the next word.",
    "mental_model": "Think of tokens as the model’s alphabet blocks. Humans see words. The model sees numerical IDs representing pieces of words.",
    "deep_dive": "Tokenization is the process of splitting text into smaller pieces. These pieces are not always full words.\n\nFor example:\n\nSentence: \"ChatGPT is amazing.\"\n\nPossible tokens:\n[\"Chat\", \"G\", \"PT\", \" is\", \" amazing\", \".\"]\n\nEach token is mapped to a number (a token ID). That number is then transformed into an embedding vector so the model can process it mathematically.\n\nImportant: Tokens are often subword units. This allows models to handle unknown words by breaking them into known pieces.\n\nCost insight: Most APIs charge per token. More tokens = more cost.",
    "applied_example": "Example:\n\nInput: \"I love machine learning\"\n\nTokenization might produce:\n[\"I\", \" love\", \" machine\", \" learning\"]\n\nThe model predicts the next token — maybe \" and\".\n\nIt does not predict an entire sentence at once. It predicts one token at a time, repeatedly.",
    "failure_modes": "1) Assuming tokens equal words.\n2) Ignoring token limits when designing prompts.\n3) Sending very long context and degrading output quality.\n4) Underestimating cost impact of token-heavy prompts.",
    "design_implications": "Good LLM system design requires:\n- Tracking token usage.\n- Managing prompt length.\n- Summarizing old context.\n- Optimizing prompts for clarity and brevity.\n\nTokens drive both performance and pricing.",
    "interview_angle": "Interview-ready framing: “Text is tokenized into subword units, mapped to token IDs, and embedded into vectors before being processed by transformer layers. LLMs generate output by predicting the next token in sequence.”\n\nIf asked why subword tokens are used: “They balance vocabulary size and flexibility, allowing models to represent rare or unseen words efficiently.”",
    "compression_summary": "Tokens are the numerical building blocks of language models. Text is split into tokens, converted into numbers, and processed one token at a time."
  },
  "quiz": [
    {
      "id": "q1",
      "type": "mcq",
      "question": "What does an LLM actually predict?",
      "options": [
        "The next sentence",
        "The next paragraph",
        "The next token",
        "The correct answer from a database"
      ],
      "correct_index": 2,
      "explanation": "LLMs generate text by predicting the next token repeatedly."
    },
    {
      "id": "q2",
      "type": "mcq",
      "question": "Why are tokens often subword units instead of full words?",
      "options": [
        "To reduce compute cost",
        "To allow flexible handling of rare or unknown words",
        "Because full words cannot be processed",
        "To avoid storing vocabulary"
      ],
      "correct_index": 1,
      "explanation": "Subword tokenization allows models to represent unknown words by combining known pieces."
    },
    {
      "id": "q3",
      "type": "mcq",
      "question": "Why do tokens matter in API usage?",
      "options": [
        "They determine user login limits",
        "They affect cost and context limits",
        "They control model training speed",
        "They change neural architecture"
      ],
      "correct_index": 1,
      "explanation": "Most LLM APIs price usage per token and enforce context limits based on token count."
    },
    {
      "id": "q4",
      "type": "short",
      "question": "Explain how a sentence becomes something a neural network can process.",
      "grading_notes": "Look for: tokenization; token IDs; embeddings; numeric vectors; transformation into model input."
    },
    {
      "id": "q5",
      "type": "short",
      "question": "Why would breaking words into subword tokens improve generalization?",
      "grading_notes": "Look for: handling rare/unseen words; smaller vocabulary; recombining known pieces."
    }
  ],
  "flashcards": [
    {
      "front": "What is a token?",
      "back": "A chunk of text processed as a unit by a language model."
    },
    {
      "front": "What does a model actually predict?",
      "back": "The next token in a sequence."
    },
    {
      "front": "Why do tokens affect cost?",
      "back": "Most APIs charge per token processed."
    },
    {
      "front": "Are tokens always full words?",
      "back": "No. They are often subword units."
    }
  ],
  "notebook_template": {
    "prompts": [
      "Take a short sentence and describe how it becomes token IDs.",
      "Explain why tokens are not the same as words.",
      "Describe one way token limits influence system design."
    ]
  }
}
